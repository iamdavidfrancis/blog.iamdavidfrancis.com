[{"categories":["development"],"content":"Since I built this blog using GitHub pages with Hugo, I thought I’d share the process I used to automate the build and deployments. GitHub pages can only serve static sites, so the only two options are to publish all the HTML/CSS/JS files to a branch or use a Jekyll theme (which GitHub has native support for). These are both great options, and I’ve used both in the past, but I really like how Hugo’s site generation and content system works.\nThe downside to using Hugo? I’d have to manually build the static content, and commit it to the gh-pages branch (Keeping main free to hold the source code that gets used by Hugo to generate the static site). I’m not a huge fan of having to do a bunch of manual steps to publish something once I finish coding (or in this case writing blog posts in Markdown), so I figured now is the perfect time to get some practice with Continuous Integration/Continuous Deployment using Azure DevOps Pipelines.\nI’ve already built the site in Hugo, got the static site defined in GitHub, and I’m using CloudFlare in front of it for the CDN and caching. The post will assume you already have a GitHub repo with a Hugo site deployed to gh-pages with the source code in main. It will cover adding automated builds for generating the Hugo site so you don’t have to do it manually.\nCI/CD Plan My goal was to have as much of the process be automated as possible. Once I push the commit to GitHub, everything else should happen automatically. The idea is to have the commit to main trigger an automated build in Azure Pipelines, then once the build finishes, have it trigger an automated Release that will push the build contents as a commit to the gh-pages branch. I also have some scripts that will invalidate the CloudFlare caches to ensure users get a fresh copy of the site when they load it after I deploy, but that’s out of scope for this post.\nSetting up Azure Pipelines to Build on New Commits Adding the Hugo Extension If you don’t already have the hugo extension installed to your Azure DevOps, you can do that here. Just click the “Get it free” button and go through the steps to install it.\nConnecting to GitHub The first thing we need to do, is set up a new Pipeline to build the static site. In the Azure DevOps project, we’ll clink on “Pipelines”, then the “New Pipeline” button. That will bring us to the “Connect” tab of the New Pipeline flow.\nWe’ll click on the “GitHub” button, which will take us to GitHub to OAuth, so that Azure DevOps can connect to the GitHub repositories.\nSelecting the Repository Once we get back to Azure DevOps, we’ll be on the “Select” tab. From this tab, we need to choose the repository we want to connect to. In my case, I’m choosing the blog’s repo:\nConfiguring the Pipeline After we click on the repo, we will go to the “Configure” tab. Lets use a “Starter pipeline” so we can add exactly what we’re going to need.\nBuilding the Pipeline YAML Now that we’ve chosen “Starter pipeline” we have arrived at the “Review” tab. Here’s where we will see the YAML file that will be used for the build definition. This file will actually be checked into the main branch of our repository and is what Azure Pipelines will use on every build. The advantage to having the build definition as code in the repo is that if we need to change how we build, we can update a file in the same place as everything else. Pushing a commit to update the build is much easier than going to the Azure DevOps site and clicking through buttons to make changes.\nBy default, the pipeline YAML file will look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # Starter pipeline# Start with a minimal pipeline that you can customize to build and deploy your code.# Add steps that build, run tests, deploy, and more:# https://aka.ms/yamltrigger:- mainpool:vmImage:'ubuntu-latest'steps:- script:echo Hello, world!displayName:'Run a one-line script'- script:|echo Add other tasks to build, test, and deploy your project. echo See https://aka.ms/yamldisplayName:'Run a multi-line script'  Our Build Definition We don’t actually want most of this, so let’s delete everything except:\n1 2  trigger:- main  Now let’s add the rest of the YAML contents we’re going to need (I’ll go over what each section is for):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  resources:- repo:selfvariables:tag:'$(Build.BuildId)'pool:vmImage:'vs2017-win2016'stages:- stage:BuilddisplayName:Build Hugo Sitejobs:- job:BuilddisplayName:Buildsteps:- checkout:selfdisplayName:'Checkout repository including submodules'submodules:true- task:HugoTask@1displayName:'Generate Hugo site'inputs:destination:'$(Build.ArtifactStagingDirectory)'baseURL:'https://blog.iamdavidfrancis.com/'- task:PublishPipelineArtifact@0displayName:'Publish Hugo site as an artifact'inputs:artifactName:'hugo-site'targetPath:'$(Build.ArtifactStagingDirectory)'  First we’re setting what resources we’re going to need in this build. The repository is going to be this repository and we’re going to build on a windows machine. We also want to tag the built artifact with the current BuildId:\n1 2 3 4 5 6 7 8  resources:- repo:selfvariables:tag:'$(Build.BuildId)'pool:vmImage:'vs2017-win2016  After we’ve set all of this, we can actually define our build pipeline. Pipelines are broken down in to individual stages, with each stage having some number of jobs, and each job having some number of steps. As you can imagine, this allows for super customized builds. Since we’re building a simple static site, we are only going to set up one stage and one job, both called ‘Build’:\n1 2 3 4 5 6 7  stages:- stage:BuilddisplayName:Build Hugo Sitesteps:- job:BuilddisplayName:Buildsteps:  We’re going to add three steps to this build:\n Checkout the repository Build the Hugo site Publish the built files as an artifact.  You’ll notice that this doesn’t actually deploy the site anywhere. We’ll set up deployment in the next section. It’s good to split up the build and deployment processes so that you can change one without needing to mess with the other. If at some point in the future we need to completely change the build process, the release process will continue to work as long as we’re publishing the same artifact.\nStep 1: Checking out the repository 1 2 3  - checkout:selfdisplayName:'Checkout repository including submodules'submodules:true  This section is fairly straightforward. We want to pull the files from the git repo, and we want to include submodules. The reason I want to include submodules, is because of how I’ve set up my Hugo site. I have added a theme to the themes folder as a submodule so I can easily update the theme when the upstream pushes new changes.\nStep 2: Building the Hugo site 1 2 3 4 5  - task:HugoTask@1displayName:'Generate Hugo site'inputs:destination:'$(Build.ArtifactStagingDirectory)'baseURL:'https://blog.iamdavidfrancis.com  Luckily someone has already built an extension for Azure DevOps to do the heavy lifting here. We don’t need to run any scripts ourselves. More information on the extension can be found here. The extension has a few inputs that are optional, but the destination is required. This is the folder where the build will output the files. We’re using Build.ArtifactStagingDirectory as this the default location Azure DevOps gives us for artifacts we want to publish.\nI’m also setting the baseURL property. This is useful as it allows us to use a different baseURL in the config.toml, which means we can put the production URL here and the config can have the URL used for local development. It also means that in a more complicated build setup where you have something like staging and production, you can change the baseURL for those as well.\nStep 3: Publishing the site as an Artifact 1 2 3 4 5  - task:PublishPipelineArtifact@0displayName:'Publish Hugo site as an artifact'inputs:artifactName:'hugo-site'targetPath:'$(Build.ArtifactStagingDirectory)'  This step will bundle all the built files and publish them in Azure DevOps as an artifact. This artifact is what we’ll use for our deployment pipeline later. For now we just need to give it an artifactName which is how we will reference it in the Release, and a targetPath that tells the task which directory to look in.\nRunning the new Build At this point, we can hit “Save and Run” in the top right corner of the window:\nThis will open a dialog asking for a commit message and which branch to commit to. This is because our build config is going to be stored as azure-pipelines.yml in the main branch. You can keep all the defaults and click the “Save and run” button on the bottom right.\nThis will take us to a page that looks like this:\nIf we click onto the “Build” job at the bottom, we will see the build steps that are being run and the logs from each step. This will tell us how long each step took, if it failed, and the output from the step. All of that can be useful for larger builds with more going on.\nOnce the build succeeds, we know we’re good to continue to make the Release.\nSetting up the Release to deploy to GitHub Pages Adding the GitHub Pages Publish Extension If you don’t already have the extension installed to your Azure DevOps, you can do that here. Just click the “Get it free” button and go through the steps to install it.\nCreating a new Release In the Azure DevOps project, we’ll clink on “Releases”, then the “New” button, then “New release pipeline” from the dropdown. That will bring us to the “New release pipeline” page.\nWe’re going to click on “Empty job” so we can add our deployment tasks manually. The first thing we’ll do is name the stage, I called mine “Deploy to GitHub Pages”:\nConfigure Artifacts Before we define the build steps, lets set up the artifacts and the build trigger. In the “Artifacts” box, we’ll click the “Add and artifact” button. In the dialog that opens, we want to choose “Build” and then populate it with the build information we set up before. In the “Source alias” box, we’ll enter the folder we want the artifacts put into. I’ve called mine site-build. Then hit the “Add” button\nThis will add an artifact to our Release pipeline:\nNow we need to click the lightning bolt at the top right, and turn on the “Continuous deployment trigger”\nConfigure Deployment GitHub Personal Access Token At this point, we’ll need a GitHub Personal Access Token (PAT) so that we can commit the files back to the gh-pages branch. Instruction on how to generate a PAT can be found here.\nWe’re going to add the PAT as a secure variable in the Release pipeline. To do that, we’ll click into the “Variables” tab, then in the “Pipeline variables” section, we’ll add a new variable called “GitHubPAT”. Make sure to toggle the padlock so that it stored as a secret. I set the Scope to the “Deploy GitHub Pages” stage to reduce the scope of access. It’s always a good idea to restrict your variable scopes to just the stages that need them.\nAdding the Deploy To GitHub Pages task. The next thing we need to do is to configure the deployment tasks. To get there we go to the “Tasks” dropdown and click on “Deploy to GitHub Pages”. From the screen that opens, click the “+” button to the right of “Agent job”. This will open an “Add tasks” window, where we will choose the “Publish to GitHub Pages” task. More information about this task, can be found here.\nOnce we add it, we’ll need to configure the settings for where it should publish. Here’s the settings we’ll want to use (You can leave “Display Name” and “Commit Message” as the defaults if you like):\n   Property Name Value Explanation     Documentation Source $(System.DefaultWorkingDirectory)\\site-build\\hugo-site\\* $(System.DefaultWorkingDirectory) is the root where all the artifacts will come in. site-build is the source alias we chose for our site, and ‘hugo-site` was the name of the artifact we published containing the site data. |   GitHub Username Your github username This is the account the extension will push the commit to.   GitHub Email Address You email address This is used as the email on the commit that gets pushed   GitHub Personal Access Token $(GitHubPAT) This is the token used to authenticate against github. Since this is a secret we want to keep secure, we’re using a variable instead of hard coding the secret here.   Repository Name Your repository name This is the repo that the extension will push the commit to.   Branch Name gh-pages This is the branch the commit will be pushed to. It should be gh-pages as that is one of the branches GitHub allows to be published as a static site.    Saving and Running the Release Now we can hit the “Save” button. It will prompt for a folder, which I have left as \\, but if you have a lot of release pipelines, it might make sense to organize them. Once we hit save, we can hit the “Create Release” button. Since this will be a manual release, there are some options that we are presented with for automated deployments and artifacts. We’ll just leave everything as the default and hit Create.\nOnce the release is created and you can watch the status as it runs:\nAfter the “Deploy to GitHub Pages” step completes successfully, the changes should show up in the browser.\nConclusion Once all of this is done, we can no check changes into our repository’s main branch, and have our changes automatically appear in the static site a few minutes later. Hopefully you found this post informative and if you have any questions or see any issues, please reach out to me on twitter.\n","description":"","tags":["github","azure devops","CI/CD"],"title":"Building a GitHub Pages site using Hugo and Azure DevOps Pipelines","uri":"/posts/hugo-static-site-github-azure-devops/"},{"categories":["development"],"content":"Sometimes when working with Asp.Net MVC or Web Apis, you’ll want to add a Filter Attribute to a class or an endpoint. This can be an Authorization Filter, a Resource Filter, an Action Filter, etc. A common use case I have seen is adding a custom Authorization Filter. In this post, I’ll go through some of the issues I’ve run into while trying to add Dependency Injection to a Filter.\nUsually a custom filter would be implemented as:\n1 2 3 4 5 6 7  public class CustomAuthorizeAttribute: AuthorizeAttribute, IAuthorizationFilter { public void OnAuthorization(AuthorizationFilterContext filterContext) { // Some authorization code.  } }   and then used like this:\n1 2 3 4 5 6  [HttpGet] [CustomAuthorize] public async IActionResult GetItem(string id) { // Some implementation. }   This allows us to move the authorization logic to a shared piece of code, to cut down on reuse and keep our controller endpoints clean. The big downside to this approach is that we can’t use Dependency Injection. If we add a constructor with arguments to the Attribute, we’ll have to pass a value for those arguments when we add the attribute to a class/method.\nLet’s say we wanted to pass in an ILogger so we could capture telemetry in our Authorization code. In our Controller we could inject it:\n1 2 3 4 5 6 7 8 9 10 11  public class SuperAwesomeController : ControllerBase { private readonly ILogger logger; public SuperAwesomeController(ILogger logger) { this.logger = logger; } }   But in our Attribute this won’t work because we would need to pass in the ILogger when we use the attribute and attribute arguments must be a compile-time constant, which it the ILogger can never be. So now we need to come up with a solution that allows us to inject services from the DI container. Asp.Net Core actually gives us two ways to do this, each with their own advantages and disadvantages: the TypeFilter and the ServiceFilter.\nTypeFilters Type filters are a convenient way to create an attribute that instantiates the attribute per request and inject services from the container. The way we do this is by making two classes, one attribute that implements TypeFilterAttribute and one that is defined as the `ImplementationType' in the Type Filter.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  public class CustomAuthorizeAttribute : TypeFilterAttribute { public CustomAuthorizeAttribute() : base(typeof(CustomAuthorizeFilter)) { } } public class CustomAuthorizeFilter : IAuthorizationFilter { private readonly ILogger logger; public CustomAuthorizeFilter(ILogger logger) { this.logger = logger; } public void OnAuthorization(AuthorizationFilterContext filterContext) { // Some authorization code.  } }   Now when we add the [CustomAuthorize] attribute to our controller, we’re actually adding a TypeFilter that will instantiate our CustomAuthorizeFilter and inject the ILogger. One of the reasons this is so powerful is that we can extend the capabilities here a little bit. Say we wanted to pass a role into the Authorization Filter to specify more granular access to the endpoint. With a traditional attribute we would implement it as:\n1 2 3 4 5 6 7 8 9 10  public class CustomAuthorizeAttribute: AuthorizeAttribute, IAuthorizationFilter { private readonly string role; public CustomAuthorizeAttribute(string role) { this.role = role; } ... }   With the power of the TypeFilter we can still do that, just with a slight modification:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  public class CustomAuthorizeAttribute : TypeFilterAttribute { public CustomAuthorizeAttribute(string role) : base(typeof(CustomAuthorizeFilter)) { Arguments = new object[] { role } } } public class CustomAuthorizeFilter : IAuthorizationFilter { private readonly string role; private readonly ILogger logger; public CustomAuthorizeFilter(string role, ILogger logger) { this.role = role; this.logger = logger; } ... }   TypeFilter Benefits and Downsides. The TypeFilter allows us to define our Filter in a way that gives us Dependency Injection and still allows us to pass in arguments like a normal Attribute would. The main downside here is that it has to instantiate an instance per request to our controller, adding some overhead to every request. If you don’t need per-instance properties like this, it might be more beneficial to use a ServiceFilter instead.\nServiceFilters Similar to a TypeFilter, a ServiceFilter allows us to add Dependency Injection to our Filters, but it goes one step further and actually pulls the Filter from our DI container, instead of making a new instance per request. This allows us to register our filter in the DI container, giving us some more control over the lifetime of it. An example implementation might look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  public class CustomAuthorizeAttribute : ServiceFilterAttribute { public CustomAuthorizeAttribute() : base(typeof(CustomAuthorizeFilter)) { } } public class CustomAuthorizeFilter : IAuthorizationFilter { private readonly ILogger logger; public CustomAuthorizeFilter(ILogger logger) { this.logger = logger; } public void OnAuthorization(AuthorizationFilterContext filterContext) { // Some authorization code.  } }   Notice how the only difference is the base type of CustomAuthorizeFilter. If we only did this though, we would get an exception as CustomAuthorizeFilter has not yet been registered in the DI container. This can easily be done by adding it to the IServiceCollection in Startup:\n1  services.AddSingleton\u003cCustomAuthorizeFilter\u003e();   This would register it as a Singleton, meaning only one instance will ever be created. You can register it with whatever scope fits your needs. Now you might be thinking, “Hey David, that’s just a normal service being added to the DI container and then using the ServiceFilterAttribute to wire the attribute up.” You would be 100% correct, that is exactly what we’re doing here. Technically you could inject any Service like this, but it really only makes sense to inject something that implements IFilterMetadata so that the framework can do its magic and run the code.\nServiceFilter Benefits and Downsides. The ServiceFilter gives us the added advantage of using a Filter registered in the DI container, saving us the overhead of creating an instance per-request. The main downside is losing the ability to pass in arguments from the Attribute like TypeFilter gives us.\nConclusion Both TypeFilter and ServiceFilter give us powerful tools for creating custom Attributes and Filters, along with the flexibility to choose the option that best suits our needs. Hopefully you found this post helpful and if you see any errors or issues, please reach out to me on twitter.\nReferences  https://docs.microsoft.com/en-us/aspnet/core/mvc/controllers/filters?view=aspnetcore-5.0 https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.mvc.typefilterattribute?view=aspnetcore-5.0 https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.mvc.servicefilterattribute?view=aspnetcore-5.0  ","description":"","tags":["dotnet","asp.net"],"title":"Using Dependency Injection in Asp.Net Core Filters \u0026 Attributes","uri":"/posts/aspnet-filter-dependency-injection/"},{"categories":["development"],"content":"Ever want to make a Discord Bot and have no idea where to get started? Yeah me too. So I spent most of a night looking into making one and setting it up in a Docker container. Some people thought it was an interesting idea, so here’s a write up. This post might get a little long, so I may split it up into a couple of posts.\n Update 2020-04-13: I realized that the Docker base image was over 500MB, which feels like too much for a bot this simple, so I switched the Docker base image to an alpine based version of node.\n The Basic Idea (A.K.A. What is this and why do I care?) Discord bots are neat. They do all kinds of things and I’ve always wondered how they were implemented. So today we’re going to build one together. Well, I’m going to build one and explain how along they way. Well, I actually already built it, but I’m building it again to explain it. Well, if you’re reading this that means I already finished building it twice, so I guess now it’s just you building it. At least you have my commentary to help! Unless my commentary is not useful, in which case, IDK there are other blogs explaining this, so go look at one of those.\nSorry, that was a long winded way of saying nothing. Anyway, here’s the plan. We’re going to make a bot that will listen on Discord and if someone says “Ping” it will reply with “Pong”. We’re going to go through the entire process of setting up the bot in Discord, writing the code, turning it into a docker image, pushing the image to Azure Container Registry and then running the docker image on a machine somewhere.\nGetting Started Before we get started, make sure you have a development environment set up. I’m using VS Code as my IDE, but any text editor will work. You’ll also need nodejs (I’m using v12.16.2 right now) and a Discord account. Get one at discordapp.com. I’ll also be doing this in Typescript since I like actually having a type system, but this can all be done in TS or JS. You will also need Docker if you want to run it in a container like I do in steps 3 \u0026 4 below.\nThe steps to creating the bot are as follows:\n Create a bot on Discord Implement the bot code in JS (or TS in our case). Create the docker image. Deploy the docker container and test the bot.  1. Create the bot on Discord. First things first, we need to create a bot in Discord and install it into one of our servers. To create the bot you need to go to the Discord Developer Portal. To create the bot, we first have to create an “application” in their system. It’s a few steps, but it’s not too complicated. You’ll need to hit the “New Application” button and choose a name for the Application. Just choose whatever name you want for this, I chose “Sample Application” and hit “Create”.\nYou will see a page that looks like this: From this page, copy down the “Client ID” as we’ll need that in a minute. Then you can click on the “Bot” link on the left to get to the bot config. You’ll need to hit “Add Bot” and confirm it first. This will create the bot for the application.\nOnce the bot has been created, you’ll need to get the token from the bot. You can do this by clicking on the “Click to Reveal Token” link and copying it, or clicking the “Copy” button. This token is going to be important later, so put it down somewhere safe. Important: This token should never be shared with anyone as it will give whoever has it full access to the API as the bot user.\nOnce you have the token saved somewhere, we need to add the bot to a server so we can actually test it in the future. We’re going to add the bot with Administrator permissions for now, but you can scope the permissions by using the tool at the bottom of the bot page to get the permission number (Administrator is 8). Once you have the number, you’ll need to construct a url to add the bot. The URL should look like this:\nhttps://discordapp.com/oauth2/authorize?\u0026client_id=CLIENTID\u0026scope=bot\u0026permissions=PERMISSION\nFor us the PERMISSION is 8, so the URL will look something like this:\nhttps://discordapp.com/oauth2/authorize?\u0026client_id=0000000000\u0026scope=bot\u0026permissions=8\nCopy the URL with your Client ID and paste it into a web browser. From there you can install the bot in a server you are an admin of. You’ll know if it worked by logging into Discord and checking if the bot user is a member of the server.\n2. Implement the bot code in JS Now that you have the bot all set up and installed in a server, we can start working on implementing it. Let’s create a folder for working on the bot. I called mine discord-bot.\n2.1 Set up package.json and tsconfig.json I’m going to launch VS Code in that folder and finish setting up from there. VS Code has a built in terminal that you can open with Ctrl+` . Let’s setup npm with npm init. Just enter information in the prompts or leave the defaults. Either is fine for now. This should create a package.json file in the file explorer. You can see that the package.json contains all the information from the prompts.\nNow we need to install some packages. Run npm install discord.js --save to add the Discord JS library to the project. Let’s also add typescript as a dev dependency with npm install typescript --save-dev. You’re going to want the Nodejs typings so run npm install @types/node @types/ws --save-dev as well. Your package.json should now look something like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  { \"name\": \"discord-bot\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"author\": \"\", \"license\": \"ISC\", \"dependencies\": { \"discord.js\": \"^12.1.1\" }, \"devDependencies\": { \"@types/node\": \"^13.11.1\", \"@types/ws\": \"^7.2.3\", \"typescript\": \"^3.8.3\" } }   Next up we need to add a tsconfig.json file to tell typescript how to build everything. Run the following command:\n1  npx tsc --init --rootDir src --outDir dist --target es6 --esModuleInterop --resolveJsonModule --module commonjs --allowJs true --noImplicitAny true   This will setup our tsconfig.json with some useful defaults. It will look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67  { \"compilerOptions\": { /* Basic Options */ // \"incremental\": true, /* Enable incremental compilation */  \"target\": \"es6\", /* Specify ECMAScript target version: 'ES3' (default), 'ES5', 'ES2015', 'ES2016', 'ES2017', 'ES2018', 'ES2019', 'ES2020', or 'ESNEXT'. */ \"module\": \"commonjs\", /* Specify module code generation: 'none', 'commonjs', 'amd', 'system', 'umd', 'es2015', 'es2020', or 'ESNext'. */ // \"lib\": [], /* Specify library files to be included in the compilation. */  \"allowJs\": true, /* Allow javascript files to be compiled. */ // \"checkJs\": true, /* Report errors in .js files. */  // \"jsx\": \"preserve\", /* Specify JSX code generation: 'preserve', 'react-native', or 'react'. */  // \"declaration\": true, /* Generates corresponding '.d.ts' file. */  // \"declarationMap\": true, /* Generates a sourcemap for each corresponding '.d.ts' file. */  // \"sourceMap\": true, /* Generates corresponding '.map' file. */  // \"outFile\": \"./\", /* Concatenate and emit output to single file. */  \"outDir\": \"dist\", /* Redirect output structure to the directory. */ \"rootDir\": \"src\", /* Specify the root directory of input files. Use to control the output directory structure with --outDir. */ // \"composite\": true, /* Enable project compilation */  // \"tsBuildInfoFile\": \"./\", /* Specify file to store incremental compilation information */  // \"removeComments\": true, /* Do not emit comments to output. */  // \"noEmit\": true, /* Do not emit outputs. */  // \"importHelpers\": true, /* Import emit helpers from 'tslib'. */  // \"downlevelIteration\": true, /* Provide full support for iterables in 'for-of', spread, and destructuring when targeting 'ES5' or 'ES3'. */  // \"isolatedModules\": true, /* Transpile each file as a separate module (similar to 'ts.transpileModule'). */  /* Strict Type-Checking Options */ \"strict\": true, /* Enable all strict type-checking options. */ \"noImplicitAny\": true, /* Raise error on expressions and declarations with an implied 'any' type. */ // \"strictNullChecks\": true, /* Enable strict null checks. */  // \"strictFunctionTypes\": true, /* Enable strict checking of function types. */  // \"strictBindCallApply\": true, /* Enable strict 'bind', 'call', and 'apply' methods on functions. */  // \"strictPropertyInitialization\": true, /* Enable strict checking of property initialization in classes. */  // \"noImplicitThis\": true, /* Raise error on 'this' expressions with an implied 'any' type. */  // \"alwaysStrict\": true, /* Parse in strict mode and emit \"use strict\" for each source file. */  /* Additional Checks */ // \"noUnusedLocals\": true, /* Report errors on unused locals. */  // \"noUnusedParameters\": true, /* Report errors on unused parameters. */  // \"noImplicitReturns\": true, /* Report error when not all code paths in function return a value. */  // \"noFallthroughCasesInSwitch\": true, /* Report errors for fallthrough cases in switch statement. */  /* Module Resolution Options */ // \"moduleResolution\": \"node\", /* Specify module resolution strategy: 'node' (Node.js) or 'classic' (TypeScript pre-1.6). */  // \"baseUrl\": \"./\", /* Base directory to resolve non-absolute module names. */  // \"paths\": {}, /* A series of entries which re-map imports to lookup locations relative to the 'baseUrl'. */  // \"rootDirs\": [], /* List of root folders whose combined content represents the structure of the project at runtime. */  // \"typeRoots\": [], /* List of folders to include type definitions from. */  // \"types\": [], /* Type declaration files to be included in compilation. */  // \"allowSyntheticDefaultImports\": true, /* Allow default imports from modules with no default export. This does not affect code emit, just typechecking. */  \"esModuleInterop\": true, /* Enables emit interoperability between CommonJS and ES Modules via creation of namespace objects for all imports. Implies 'allowSyntheticDefaultImports'. */ // \"preserveSymlinks\": true, /* Do not resolve the real path of symlinks. */  // \"allowUmdGlobalAccess\": true, /* Allow accessing UMD globals from modules. */  /* Source Map Options */ // \"sourceRoot\": \"\", /* Specify the location where debugger should locate TypeScript files instead of source locations. */  // \"mapRoot\": \"\", /* Specify the location where debugger should locate map files instead of generated locations. */  // \"inlineSourceMap\": true, /* Emit a single file with source maps instead of having a separate file. */  // \"inlineSources\": true, /* Emit the source alongside the sourcemaps within a single file; requires '--inlineSourceMap' or '--sourceMap' to be set. */  /* Experimental Options */ // \"experimentalDecorators\": true, /* Enables experimental support for ES7 decorators. */  // \"emitDecoratorMetadata\": true, /* Enables experimental support for emitting type metadata for decorators. */  /* Advanced Options */ \"resolveJsonModule\": true, /* Include modules imported with '.json' extension */ \"forceConsistentCasingInFileNames\": true /* Disallow inconsistently-cased references to the same file. */ } }   Now that this is done, we can make a few changes to package.json to enable some easier testing and running. Let’s update the \"main\" property to dist/index.js since we won’t be running anything from /src. Let’s also add some scripts to enable building and running our bot code:\n6 7 8 9 10  \"scripts\": { \"build\": \"tsc\", \"start\": \"node dist/index.js\", \"debug\": \"node --inspect dist/index.js\" }   Your package.json should look like this now:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  { \"name\": \"discord-bot\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"dist/index.js\", \"scripts\": { \"build\": \"tsc\", \"start\": \"node dist/index.js\", \"debug\": \"node --inspect dist/index.js\" }, \"author\": \"\", \"license\": \"ISC\", \"dependencies\": { \"discord.js\": \"^12.1.1\" }, \"devDependencies\": { \"@types/node\": \"^13.11.1\", \"typescript\": \"^3.8.3\" } }   2.2 Adding the bot entry point and connecting to Discord Now that all we’ve got typescript and node set up, let’s get started on implementation. Let’s create a src folder and put an index.ts file in it. The main part of this file will be creating the Discord client and connecting to Discord with it. Note: Discord.js has a ton of built in functionality and is too much to cover in this blog post. You can read the full docs here. I’ll cover the code in the file by blocks, so the first thing we want to do is add this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import Discord from \"discord.js\"; // Discord token is required. if (!process.env.DISCORD_TOKEN) { throw new Error(\"DISCORD_TOKEN environment variable missing.\"); } const onReady = () =\u003e { // TODO: Implement this }; const onMessage = (message: Discord.Message) =\u003e { // TODO: Implement this }; const client = new Discord.Client(); client.on('ready', onReady); client.on('message', onMessage); const discordToken: string = process.env.DISCORD_TOKEN; client.login(discordToken);   This will bring in the Discord library under the name Discord. It also will read in the token from an Environment variable. I don’t like hard coding secrets, especially if I’m checking them into source control. So let’s throw if the token is not in the environment and set it to a local variable while we do the rest.\nThe onReady and onMessage handlers will be invoked once the client starts up and when incoming messages come in. We’ll actually implement them a little later.\nThe client is actually the magic that will connect to Discord and calls into our onReady and onMessage callbacks.\nThe call to client.login(discordToken) will initiate the connection to Discord. At this point, we can add some logging into the onReady handler and see what happens when we run it. Update the code with the following:\n8 9 10 11 12 13 14  const onReady = () =\u003e { console.log(\"Connected\"); if (client.user) { console.log(`Logged in as ${client.user.tag}.`); } }   Once that’s in, run npm run build to generate the dist folder with the generated javascript. Once that completes, run npm run start to start the bot running locally. You should see something along the lines of this:\nPS D:\\temp-blog\\discord-bot\u003e npm run start\r\u003e discord-bot@1.0.0 start D:\\temp-blog\\discord-bot\r\u003e node dist/index.js\rD:\\temp-blog\\discord-bot\\dist\\index.js:9\rthrow new Error(\"DISCORD_TOKEN environment variable missing.\");\r^\rError: DISCORD_TOKEN environment variable missing.\rThis is because we haven’t set the environment variable with our token yet. I’m using Powershell, so I’m going to run:\n1  $env:DISCORD_TOKEN = 'MyToken'   If you run npm run start again you should now see the log lines when you bot connects:\nPS D:\\temp-blog\\discord-bot\u003e npm run start\r\u003e discord-bot@1.0.0 start D:\\temp-blog\\discord-bot\r\u003e node dist/index.js\rConnected\rLogged in as Sample Application#2809.\rGo ahead and hit Ctrl+C to stop the process.\n2.3 Handling incoming messages. Now that we are connected to Discord, we can implement our message handling. As I said earlier, this bot will just respond “Pong” when someone sends “Ping”, so let’s build a pretty simple handler.\nIn our onMessage method let’s add our new code:\n15 16 17 18 19 20 21 22 23 24  const onMessage = (message: Discord.Message) =\u003e { // Don't respond to bots.  if (message.author.bot) { return; } if (message.content.toLowerCase() == \"ping\") { message.reply(\"Pong!\"); } }   Now you can run npm run build and npm run start again. Once you see the “Connected” log show up, go back to your Discord server and send a message “Ping”. You should see this response:\nNow we’re ready to start working on the Docker Image.\n3. Create the Docker Image The first step to creating the Docker image will be to add a Dockerfile. Lets create a Dockerfile at the root of our discord-bot folder. Set the content of the Dockerfile to this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  FROMnode:lts-alpine3.9USERrootENV APP /usr/src/APPCOPY package.json /tmp/package.jsonRUN cd /tmp \u0026\u0026 npm install --loglevel=warn \\  \u0026\u0026 mkdir -p $APP \\  \u0026\u0026 mv /tmp/node_modules $APPCOPY src $APP/srcCOPY package.json $APPCOPY tsconfig.json $APPWORKDIR$APPRUN npm run buildCMD [ \"node\", \"dist/index.js\" ]  Let’s break down what we’re doing in there. We start with setting the base image to node:lts-alpine3.9 to get the current LTS nodejs in our container. We’re using an alpine linux based image, which is much smaller while still giving us everything we need.\n3 4  USERrootENV APP /usr/src/APP  Here we’re setting the user to run as and adding an environment variable with the path we’re installing the bot to.\n6 7 8 9 10  COPY package.json /tmp/package.jsonRUN cd /tmp \u0026\u0026 npm install --loglevel=warn \\  \u0026\u0026 mkdir -p $APP \\  \u0026\u0026 mv /tmp/node_modules $APP  This is copying the package.json into a temp folder to run an npm install to download our dependencies. Then it copies the dependencies into our $APP directory.\n12 13 14 15 16  COPY src $APP/srcCOPY package.json $APPCOPY tsconfig.json $APPWORKDIR$APP  This is copying app files into the final directory and moving our working directory there.\n18 19 20  RUN npm run buildCMD [ \"node\", \"dist/index.js\" ]  This builds the bot and then runs it. Now let’s actually build our docker image. Go ahead and run this command:\n1  docker build . -t discord-bot   This will build the image and tag it with discord-bot. You can you another tag if you like. Now you can try running your docker image. Remember to pass in your token via the -e flag. Here’s what my command looks like:\n1  docker run -e DISCORD_TOKEN=$env:DISCORD_TOKEN docker-bot   4. Deploy the docker container and test the bot. To deploy the container off of your local machine, you first need to push it to your registry. I’m going to use Azure Container Registry (ACR) since I already have one. You could always publish to DockerHub, but it was easier for me to use my existing registry. If you’re interested in using Azure, instructions for setting up a new registry in Azure can be found here.\nSince I’m using ACR for this, I need to tag my image a little differently, so I’m going to run these commands to push it:\n1 2  docker tag discord-bot mysupercoolregistry.azurecr.io/discord-bot docker push mysupercoolregistry.azurecr.io/discord-bot:latest   Now that it’s deployed, I can go to my Docker VM host and run the following command to run the image:\n1  docker run -e DISCORD_TOKEN=\"$env:DISCORD_TOKEN\" --restart unless-stopped mysupercoolregistry.azurecr.io/discord-bot:latest   And that’s it, we’re all set up and running. The bot will always run unless I manually turn it off in Docker. Thanks for reading all the way through. If I feel like it, I’ll put up a blog post about how I set up CI/CD to deploy new updates to ACR using Azure DevOps Pipelines and GitHub.\n","description":"","tags":["js","docker"],"title":"Creating a Discord Bot with Docker","uri":"/posts/discord-bot/"},{"categories":["meta"],"content":"Hello there! Welcome to my blog. I can’t believe I’m doing this, but here we are. If you think this is dumb and I didn’t need a blog, same. Someone asked my to write up a blog post about a personal project I did, so here we are. Honestly this was a lot of work and I’d much rather be sleeping, but sleeping never got me anywhere so screw that. Don’t expect a serious tone from me as I’m like one step above “I’m here so I don’t get fined.” I know you’re going to ask question like “Isn’t this a personal blog?” and “It’s not like you’re being forced to write it right?” and while those are both correct, it really undercuts the constant complaining motif I was going for.\nDon’t expect a lot of updates, I’ve never been able to do that consistently. I figure if I set your expectations low enough now, I will have less disappointment later. So, to recap:\n Don’t expect a lot of posts Don’t expect a serious tone all the time. Don’t be a dick.  Thanks for reading, I guess. Honestly I’m surprised you made it this far. I guess I’m not the only one with nothing better to do.\n","description":"","tags":null,"title":"Hello World","uri":"/posts/hello-world/"},{"categories":null,"content":"So I guess this is where I talk about myself? Weird. I’m a Software Engineer and sometimes people seem interested in what I say. If you are interested, this blog is for you I guess. If not, I feel ya.\nDon’t expect a lot of content here, but who knows what will happen. Maybe I’ll get bored again and actually post something here again. Anyway, here’s Wonderwall.\nFull Disclosure: I’m a Software Engineer at Microsoft. While I tend to prefer using Azure tooling over other cloud providers, this is due to my familiarity with it. Any opinions shared on this site are mine and do not reflect my employer’s views in any way.\n","description":"","tags":null,"title":"About David Francis","uri":"/about/"}]
